{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e53a561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: accelerate 1.8.1\n",
      "Uninstalling accelerate-1.8.1:\n",
      "  Successfully uninstalled accelerate-1.8.1\n",
      "Found existing installation: transformers 4.53.2\n",
      "Uninstalling transformers-4.53.2:\n",
      "  Successfully uninstalled transformers-4.53.2\n",
      "Collecting transformers==4.36.0\n",
      "  Downloading transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
      "Collecting accelerate==0.26.1\n",
      "  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from transformers==4.36.0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from transformers==4.36.0) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from transformers==4.36.0) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from transformers==4.36.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from transformers==4.36.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from transformers==4.36.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from transformers==4.36.0) (2.32.4)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.0)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from transformers==4.36.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from transformers==4.36.0) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from accelerate==0.26.1) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from accelerate==0.26.1) (2.7.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0) (1.1.5)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.26.1) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from triton==3.3.1->torch>=1.10.0->accelerate==0.26.1) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.26.1) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.1) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from requests->transformers==4.36.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from requests->transformers==4.36.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from requests->transformers==4.36.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages (from requests->transformers==4.36.0) (2025.7.9)\n",
      "Downloading transformers-4.36.0-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
      "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers, accelerate\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.21.2\n",
      "\u001b[2K    Uninstalling tokenizers-0.21.2:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.21.2\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [accelerate]3\u001b[0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-0.26.1 tokenizers-0.15.2 transformers-4.36.0\n"
     ]
    }
   ],
   "source": [
    "# Force reinstall with specific versions\n",
    "!pip uninstall accelerate transformers -y\n",
    "!pip install transformers==4.36.0 accelerate==0.26.1 --no-cache-dir\n",
    "!pip install torch datasets scikit-learn pandas --quiet\n",
    "\n",
    "# Restart kernel after installation if in Jupyter/Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e70f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3090 Ti\n",
      "Memory: 22.49 GB\n",
      "TF32 enabled for faster training on 3090 Ti\n",
      "Loading existing balanced dataset...\n",
      "Loaded 1311735 balanced samples\n",
      "\n",
      "Label mapping: {'Informational': 0, 'Navigational': 1, 'Transactional': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arya/projects/wb_hack_sf/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training samples: 1180561\n",
      "Validation samples: 131174\n",
      "Tokenizing dataset in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 237/237 [00:44<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready! Tensors on cuda\n",
      "GPU memory used by dataset: 1.13 GB\n",
      "Tokenizing dataset in batches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 27/27 [00:04<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready! Tensors on cuda\n",
      "GPU memory used by dataset: 0.13 GB\n",
      "\n",
      "DataLoader info:\n",
      "Training batches per epoch: 2362\n",
      "Validation batches per epoch: 263\n",
      "All data pre-loaded on GPU - no CPU-GPU transfer needed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_71463/2688762284.py:292: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # For mixed precision training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Training with batch size: 500\n",
      "Steps per epoch: 2362\n",
      "Total training steps: 11810\n",
      "GPU memory before training: 1.73 GB\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2362 [00:00<?, ?it/s]/tmp/ipykernel_71463/2688762284.py:220: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Training:  48%|████▊     | 1142/2362 [07:49<07:39,  2.65it/s, loss=0.2760, acc=0.8093, lr=9.67e-05, gpu_mem=3.1GB]"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Check if GPU is available and set up mixed precision\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    # Enable TF32 on Ampere GPUs (3090 Ti)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(\"TF32 enabled for faster training on 3090 Ti\")\n",
    "\n",
    "# Load and prepare the dataset\n",
    "import os\n",
    "\n",
    "# Check if balanced dataset already exists\n",
    "if os.path.exists(\"balanced_orcas_dataset.tsv\"):\n",
    "    print(\"Loading existing balanced dataset...\")\n",
    "    df = pd.read_csv(\"balanced_orcas_dataset.tsv\", sep=\"\\t\")\n",
    "    balanced_df = df\n",
    "    print(f\"Loaded {len(df)} balanced samples\")\n",
    "else:\n",
    "    print(\"Downloading and creating balanced dataset...\")\n",
    "    url = \"https://researchdata.tuwien.ac.at/records/pp7xz-n9a06/files/ORCAS-I-18M.tsv?download=1\"\n",
    "    \n",
    "    # Check if original dataset exists\n",
    "    if os.path.exists(\"ORCAS-I-18M.tsv\"):\n",
    "        print(\"Loading existing original dataset...\")\n",
    "        df = pd.read_csv(\"ORCAS-I-18M.tsv\", sep=\"\\t\")\n",
    "    else:\n",
    "        print(\"Downloading original dataset...\")\n",
    "        df = pd.read_csv(url, sep=\"\\t\")\n",
    "        # Save original dataset for future use\n",
    "        df.to_csv(\"ORCAS-I-18M.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "    # Filter to necessary columns\n",
    "    df = df[[\"query\", \"level_1\"]].dropna().drop_duplicates()\n",
    "\n",
    "    # Count samples in each category\n",
    "    category_counts = df[\"level_1\"].value_counts()\n",
    "    print(\"\\nOriginal counts:\")\n",
    "    print(category_counts)\n",
    "\n",
    "    # Find the lowest category and sample equally\n",
    "    lowest = category_counts.min()\n",
    "    print(f\"\\nLowest category count: {lowest}\")\n",
    "\n",
    "    # Sample equal numbers from each category\n",
    "    balanced_dfs = []\n",
    "    for category in category_counts.index:\n",
    "        balanced_dfs.append(\n",
    "            df[df[\"level_1\"] == category].sample(n=lowest, random_state=42)\n",
    "        )\n",
    "\n",
    "    # Combine the balanced datasets\n",
    "    balanced_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "\n",
    "    print(f\"\\nBalanced dataset size: {len(balanced_df)}\")\n",
    "    print(\"Category distribution:\")\n",
    "    print(balanced_df[\"level_1\"].value_counts())\n",
    "\n",
    "    # Save the balanced dataset\n",
    "    balanced_df.to_csv(\"balanced_orcas_dataset.tsv\", sep=\"\\t\", index=False)\n",
    "    print(\"Balanced dataset saved.\")\n",
    "\n",
    "# Prepare for training\n",
    "df = balanced_df\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"level_1\"])\n",
    "label_map = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
    "print(\"\\nLabel mapping:\", label_map)\n",
    "\n",
    "# Split train/val\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"query\"].tolist(),\n",
    "    df[\"label\"].tolist(),\n",
    "    test_size=0.1,\n",
    "    stratify=df[\"label\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Dataset class\n",
    "# Dataset class with optimized tokenization and caching\n",
    "class IntentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=64, device='cuda'):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.max_len = max_len\n",
    "        self.device = device\n",
    "        \n",
    "        # Batch tokenization for efficiency\n",
    "        print(\"Tokenizing dataset in batches...\")\n",
    "        batch_size = 5000  # Process in chunks\n",
    "        all_input_ids = []\n",
    "        all_attention_masks = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Tokenizing\"):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            encodings = tokenizer(\n",
    "                batch_texts,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=max_len,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Move to GPU immediately to save CPU-GPU transfer time later\n",
    "            if device == 'cuda':\n",
    "                all_input_ids.append(encodings['input_ids'].to(device))\n",
    "                all_attention_masks.append(encodings['attention_mask'].to(device))\n",
    "            else:\n",
    "                all_input_ids.append(encodings['input_ids'])\n",
    "                all_attention_masks.append(encodings['attention_mask'])\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        self.input_ids = torch.cat(all_input_ids, dim=0)\n",
    "        self.attention_masks = torch.cat(all_attention_masks, dim=0)\n",
    "        \n",
    "        # Move labels to GPU if needed\n",
    "        if device == 'cuda':\n",
    "            self.labels = self.labels.to(device)\n",
    "        \n",
    "        print(f\"Dataset ready! Tensors on {device}\")\n",
    "        if device == 'cuda':\n",
    "            print(f\"GPU memory used by dataset: {(self.input_ids.element_size() * self.input_ids.nelement() + self.attention_masks.element_size() * self.attention_masks.nelement()) / 1024**3:.2f} GB\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Everything is already on GPU, just index\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Create datasets with GPU tensors\n",
    "train_dataset = IntentDataset(train_texts, train_labels, tokenizer, device=device.type)\n",
    "val_dataset = IntentDataset(val_texts, val_labels, tokenizer, device=device.type)\n",
    "\n",
    "# Create data loaders - data is already on GPU!\n",
    "# Use larger batches since no CPU-GPU transfer overhead\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=500,   # Even larger now\n",
    "    shuffle=True,\n",
    "    num_workers=0,     # No workers needed - data is on GPU\n",
    "    pin_memory=False   # Not needed - already on GPU\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=500,   # Very large for validation\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\"\\nDataLoader info:\")\n",
    "print(f\"Training batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Validation batches per epoch: {len(val_loader)}\")\n",
    "print(f\"All data pre-loaded on GPU - no CPU-GPU transfer needed!\")\n",
    "\n",
    "# Initialize model\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\n",
    "model.to(device)\n",
    "\n",
    "# Check if a trained model already exists\n",
    "model_path = './best_intent_classifier'\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"\\nFound existing model at {model_path}\")\n",
    "    user_input = input(\"Do you want to load the existing model instead of training? (y/n): \")\n",
    "    if user_input.lower() == 'y':\n",
    "        print(\"Loading existing model...\")\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "        model.to(device)\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "        skip_training = True\n",
    "    else:\n",
    "        skip_training = False\n",
    "else:\n",
    "    skip_training = False\n",
    "\n",
    "# Manual Training Implementation with GPU-resident data\n",
    "def train_epoch(model, data_loader, optimizer, scheduler, device, scaler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Data is already on GPU, no transfer needed!\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        # Mixed precision training\n",
    "        with autocast():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()  # Update learning rate\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar with more info\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{correct_predictions / total_predictions:.4f}',\n",
    "            'lr': f'{current_lr:.2e}',\n",
    "            'gpu_mem': f'{torch.cuda.memory_allocated() / 1024**3:.1f}GB'\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(data_loader), correct_predictions / total_predictions\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            # Data already on GPU\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            correct_predictions += (predictions == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader), correct_predictions / total_predictions\n",
    "\n",
    "# Training configuration optimized for 3090 Ti\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)  # Higher LR for larger batches\n",
    "num_epochs = 5\n",
    "scaler = GradScaler()  # For mixed precision training\n",
    "\n",
    "# Learning rate scheduler for better convergence\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(0.1 * total_steps),  # 10% warmup\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "if not skip_training:\n",
    "    print(\"\\nStarting training...\")\n",
    "    print(f\"Training with batch size: {train_loader.batch_size}\")\n",
    "    print(f\"Steps per epoch: {len(train_loader)}\")\n",
    "    print(f\"Total training steps: {len(train_loader) * num_epochs}\")\n",
    "    \n",
    "    # Monitor GPU usage\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    \n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, scheduler, device, scaler)\n",
    "        print(f\"Training - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU memory usage: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "            print(f\"GPU utilization: Check nvidia-smi\")\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "        print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            model.save_pretrained('./best_intent_classifier')\n",
    "            tokenizer.save_pretrained('./best_intent_classifier')\n",
    "            print(f\"Saved best model with validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Save final model\n",
    "    model.save_pretrained('./intent_classifier_final')\n",
    "    tokenizer.save_pretrained('./intent_classifier_final')\n",
    "    print(\"\\n✓ Training completed! Final model saved.\")\n",
    "    print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "else:\n",
    "    print(\"\\nSkipping training, using loaded model.\")\n",
    "    # Still evaluate the loaded model\n",
    "    val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "    print(f\"Loaded model validation - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# Test the model\n",
    "def predict_intent(text, model, tokenizer, label_encoder, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=32\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "        predicted_label = label_encoder.inverse_transform([prediction])[0]\n",
    "        \n",
    "        # Get confidence scores\n",
    "        probs = torch.softmax(outputs.logits, dim=-1).squeeze().cpu().numpy()\n",
    "        \n",
    "    return predicted_label, probs\n",
    "\n",
    "# Test with sample queries\n",
    "test_queries = [\n",
    "    \"github\",\n",
    "    \"how to cook pasta\",\n",
    "    \"buy shoes online\",\n",
    "    \"weather forecast tomorrow\",\n",
    "    \"login to my account\",\n",
    "    \"facebook\",\n",
    "    \"best laptop under $1000\",\n",
    "    \"amazon prime membership\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing model with sample queries:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for query in test_queries:\n",
    "    predicted_label, probs = predict_intent(query, model, tokenizer, label_encoder, device)\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(f\"Prediction: {predicted_label}\")\n",
    "    print(f\"Confidence scores:\")\n",
    "    for label, prob in zip(label_encoder.classes_, probs):\n",
    "        print(f\"  {label}: {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad80b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
