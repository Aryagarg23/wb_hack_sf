# -*- coding: utf-8 -*-
"""ROBERTA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sbDCWGWl6X2pqRuNwIKlddM1P0pBro6m

Use ORCAS-I-2m.tsv at https://researchdata.tuwien.ac.at/records/pp7xz-n9a06
"""

!pip install pandas torch transformers scikit-learn datasets --quiet

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Load the TSV data
df = pd.read_csv("ORCAS-I-2M.tsv", sep="\t")

df.info()
df.head()

# Filter to necessary columns
df = df[["query", "level_1"]].dropna().drop_duplicates()

# Encode Level 1 labels (Nav/Info/Trans)
label_encoder = LabelEncoder()
df["label"] = label_encoder.fit_transform(df["level_1"])
label_map = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))
print("Label mapping:", label_map)

# Split train/val
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df["query"].tolist(),
    df["label"].tolist(),
    test_size=0.1,
    stratify=df["label"],
    random_state=42
)

# Tokenizer and dataset
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

class IntentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=32):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encodings = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt'
        )
        return {
            'input_ids': encodings['input_ids'].squeeze(),
            'attention_mask': encodings['attention_mask'].squeeze(),
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }

train_dataset = IntentDataset(train_texts, train_labels, tokenizer)
val_dataset = IntentDataset(val_texts, val_labels, tokenizer)

# Model and Trainer
model = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=3)

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=30,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    warmup_steps=50,
    weight_decay=0.01,
    eval_strategy="epoch",
    logging_dir='./logs',
    logging_steps=10,
    save_strategy="epoch",
    load_best_model_at_end=True
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer
)

# Train
trainer.train()

